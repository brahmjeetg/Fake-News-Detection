# -*- coding: utf-8 -*-
"""Fake News Detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Jzy1MSWCmaDQrzPvlPnKJ8VKN4Kk10L
"""

import pandas as pd
import tensorflow as tf 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS
import nltk
import re

from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS

!pip install jupyterthemes

from tensorflow.keras.preprocessing.text import one_hot, Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import models, layers
from jupyterthemes import jtplot

jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)

df_true = pd.read_csv('/content/drive/My Drive/Colab Dataset/True.csv')
df_fake = pd.read_csv('/content/drive/My Drive/Colab Dataset/Fake.csv')

df_true.isnull().sum()

df_fake.isnull().sum()

df_true['isfake'] = 0
df_true.head()

df_fake['isfake'] = 1
df_fake.head()

df = pd.concat([df_true, df_fake]).reset_index(drop=True)
df

df.drop(columns = ['date'], inplace = True)

df['original'] = df['title'] + '' + df['text']

df.head()

df['original'][0]

nltk.download('stopwords')

from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])
stop_words

def cleandata(text):
  result = []
  for token in gensim.utils.simple_preprocess(text):
    if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:
      result.append(token)

  return result

df['clean'] = df['original'].apply(cleandata)

df

df['original'][0]

print(df['clean'][0])

list_of_words = []
for i in df.clean:
  for j in i:
    list_of_words.append(i)

list_of_words

len(list_of_words)

df['clean_joined'] = df['clean'].apply(lambda x: " ".join(x))

df

df['clean_joined'][0]

plt.figure(figsize=(8,8))
sns.countplot(y = "isfake", data=df)

plt.figure(figsize=(20, 20))
wc = WordCloud(max_words=2000, width=1600, height=800, stopwords=stop_words).generate(" ".join(df[df.isfake==1].clean_joined))
plt.imshow(wc, interpolation='bilinear')

plt.figure(figsize=(20, 20))
wc = WordCloud(max_words=2000, width=1600, height=800, stopwords=stop_words).generate(" ".join(df[df.isfake==0].clean_joined))
plt.imshow(wc, interpolation='bilinear')

nltk.download('punkt')

nltk.word_tokenize(df['clean_joined'][0])

maxlen = -1
for doc in df.clean_joined:
  tokens = nltk.word_tokenize(doc)
  if(maxlen<len(tokens)):
    maxlen = len(tokens)
print("The maximum number of words in any document is =", maxlen)

import plotly.express as px

fig = px.histogram(x = [len(nltk.word_tokenize(x)) for x in df.clean_joined], nbins=100)
fig.show()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(df.clean_joined, df.isfake, test_size=0.2)

from nltk import word_tokenize

total_words = len(df.clean_joined)
tokenizer = Tokenizer(num_words = total_words)
tokenizer.fit_on_texts(x_train)

len(train_sequence)

